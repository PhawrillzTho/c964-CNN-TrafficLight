{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.patches as patches\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense,Activation,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration - Whats in the box csv file?\n",
    "day_clip6_anno_path_box = \"./data/sample-dayClip6/sample-dayClip6/frameAnnotationsBOX.csv\"\n",
    "\n",
    "pd.read_csv(day_clip6_anno_path_box, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration - Whats the difference between the BOX and BULB files?\n",
    "day_clip6_anno_path_bulb = \"./data/sample-dayClip6/sample-dayClip6/frameAnnotationsBULB.csv\"\n",
    "\n",
    "pd.read_csv(day_clip6_anno_path_bulb, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd960823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What variety of tags do we have for our model?\n",
    "\n",
    "dayTrainFolder = \"./data/Annotations/Annotations/dayTrain/\"\n",
    "annotation_folders = os.listdir(dayTrainFolder)\n",
    "\n",
    "for folder in annotation_folders:\n",
    "    annotation_file = dayTrainFolder+folder+\"/frameAnnotationsBOX.csv\"\n",
    "    print('-----From folder:', folder)\n",
    "    print((pd.read_csv(annotation_file, sep=';'))['Annotation tag'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to know the total annotations found in each folder. This is relavant for when we train our model.\n",
    "\n",
    "dayTrainFolder = \"./data/Annotations/Annotations/dayTrain/\"\n",
    "annotation_folders = os.listdir(dayTrainFolder)\n",
    "\n",
    "annotation_totals = {'stop':0, 'go':0, 'stopLeft':0, 'goLeft':0, 'warningLeft':0, 'warning':0}\n",
    "\n",
    "for folder in annotation_folders:\n",
    "    annotation_file = dayTrainFolder+folder+\"/frameAnnotationsBOX.csv\"\n",
    "    #print((pd.read_csv(annotation_file, sep=';'))['Annotation tag'].value_counts())\n",
    "    item = (pd.read_csv(annotation_file, sep=';'))['Annotation tag'].value_counts()\n",
    "    try:\n",
    "        if item.stop:\n",
    "            annotation_totals['stop'] += item.stop\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if item.go:\n",
    "            annotation_totals['go'] += item.go\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if item.stopLeft:\n",
    "            annotation_totals['stopLeft'] += item.stopLeft\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if item.goLeft:\n",
    "            annotation_totals['goLeft'] += item.goLeft\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if item.warningLeft:\n",
    "            annotation_totals['warningLeft'] += item.warningLeft\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if item.warning:\n",
    "            annotation_totals['warning'] += item.warning\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(annotation_totals)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see it in a graph\n",
    "plt.bar(range(len(annotation_totals)), list(annotation_totals.values()), align='center')\n",
    "plt.xticks(range(len(annotation_totals)), list(annotation_totals.keys()))\n",
    "\n",
    "print(\"Total annotations found by category:\")\n",
    "plt.show()\n",
    "\n",
    "# I am starting to think I wont have enough images in these categories to sufficiently train the model accross them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e27926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring it all together... This takes about 40 minutes, sooo...dont run it unless you have to. \n",
    "# or at least remove some of the frames folders\n",
    "\n",
    "frame_path_arr = []\n",
    "annotations = []\n",
    "images = []\n",
    "labels = []\n",
    "color_space = [(0,255,0),(255,0,0),(255,0,0)]\n",
    "\n",
    "def process_images():\n",
    "    for folder in os.listdir(\"./data/dayTrain/dayTrain/\"):\n",
    "        subfolder = os.listdir(\"./data/dayTrain/dayTrain/\"+folder+\"/\"+\"/frames\")\n",
    "        \n",
    "        for file in subfolder:\n",
    "            frame_path_arr.append(\"./data/dayTrain/dayTrain/\"+folder+\"/\"+\"/frames/\"+file)\n",
    "\n",
    "    for img_path in frame_path_arr:\n",
    "        frame_name = img_path[::-1][0:img_path[::-1].index(\"/\")][::-1]\n",
    "        #print(frame_name)\n",
    "        frame_anno = \"./data/Annotations/Annotations/dayTrain\"+\"/\"+img_path[25:34].replace(\"/\",\"\")+\"/frameAnnotationsBOX.csv\"\n",
    "        #print(frame_anno)\n",
    "        \n",
    "        with open(frame_anno) as fa:  \n",
    "            line = fa.readline()\n",
    "            line = fa.readline()\n",
    "            while line:\n",
    "                anno_file_path = (line.strip()).split(\";\")\n",
    "                anno_file_id = anno_file_path[0].split(\"/\")[1]\n",
    "                #print(anno_file_path, anno_file_id)\n",
    "                #print(anno_file_id, frame_name)\n",
    "                if anno_file_id == frame_name:\n",
    "                    annotations.append(anno_file_path)\n",
    "                line = fa.readline()\n",
    "\n",
    "    for img_path in frame_path_arr:\n",
    "        for anno in annotations:\n",
    "            #print(img_path[::-1][0:img_path[::-1].index(\"/\")][::-1])\n",
    "            #print(str(anno).split(\",\")[0].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"dayTraining/\", \"\"))\n",
    "            if img_path[::-1][0:img_path[::-1].index(\"/\")][::-1] == str(anno).split(\",\")[0].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"dayTraining/\", \"\"):\n",
    "                category = anno[1]\n",
    "                left = int(anno[2])\n",
    "                top = int(anno[3])\n",
    "                right = int(anno[4])\n",
    "                bottom = int(anno[5])\n",
    "                #print(\"file:\", img_path, \"\\tCategory: '{}' at [{},{},{},{}]\".format(category, left, top, right, bottom))\n",
    "                img = cv2.imread(img_path)\n",
    "                #img = cv2.resize(img, (25, 25))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                #plt.rcParams['figure.figsize'] = [32, 16]\n",
    "                cropped_image = img[top:bottom, left:right]\n",
    "                cropped_image = cv2.resize(cropped_image, (25, 25))\n",
    "                \n",
    "                try:\n",
    "                    if category == \"stop\":\n",
    "                        labels.append(0)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"go\":\n",
    "                        labels.append(1)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"stopLeft\":\n",
    "                        labels.append(0)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"goLeft\":\n",
    "                        labels.append(1)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"warningLeft\":\n",
    "                        labels.append(2)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"warning\":\n",
    "                        labels.append(2)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                images.append(cropped_image)\n",
    "                print(\"Working on: \", img_path[::-1][0:img_path[::-1].index(\"/\")][::-1])\n",
    "                # --- uncomment to show images ---\n",
    "                cv2.imshow(\"cropped\", cropped_image)\n",
    "                plt.imshow(cropped_image)\n",
    "                plt.show()   \n",
    "                \n",
    "\n",
    "process_images()\n",
    "\n",
    "data = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total images and labels:\n",
    "print(\"Images: \", len(images))\n",
    "print(\"Labels: \", len(labels))\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(\"Shapes pre-split\", data.shape, labels.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "print(\"Shapes post-split\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "#print(train_test_split(data, labels, test_size=0.2, random_state=42))\n",
    "#print(y_train)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1770499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think 15 epochs is going to over fit it... lets see though\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=6000, epochs=15, validation_data=(X_test, y_test))\n",
    "model.save(\"C964_TrafficLightModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ba75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting graphs for accuracy \n",
    "plt.figure(0)\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b345e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211254c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets test the model with images its never seen before\n",
    "\n",
    "# Lets look at our results\n",
    "\n",
    "test_files=os.listdir(\"./data/MyTestImages\")\n",
    "\n",
    "CATEGORIES = [\"Red\", \"Green\"]\n",
    "\n",
    "model = tf.keras.models.load_model(\"C964_TrafficLightModel.h5\")\n",
    "\n",
    "accuracy_total = {'Green':0, 'Red':0}\n",
    "total_red = 0\n",
    "total_yellow = 0\n",
    "total_green = 0\n",
    "total_red_correct_predict = 0\n",
    "total_yellow_correct_predict = 0\n",
    "total_green_correct_predict = 0\n",
    "\n",
    "\n",
    "def prep_img(file):\n",
    "    img = cv2.imread(file)\n",
    "    new_img = cv2.resize(img, (25, 25))\n",
    "    final_img = new_img.reshape(1, 25, 25, 3)\n",
    "    #plt.imshow(final_img)\n",
    "    #plt.show()\n",
    "    #print(new_img.shape)\n",
    "    return final_img\n",
    "\n",
    "\n",
    "index = 0\n",
    "for file in test_files:\n",
    "    prediction = model.predict(prep_img(\"./data/MyTestImages/\"+file))\n",
    "    #print(prediction)\n",
    "    print(\"File\", file, \"has been predicted as being: \",\n",
    "         \n",
    "         \"{:.2f}\".format(prediction[0][0]*100), \"% Red - \",\n",
    "          \"{:.2f}\".format(prediction[0][1]*100), \"% Green - \"\n",
    "          #,\"{:.2f}\".format(prediction[0][2]*100), \" % Yellow\"\n",
    "         \n",
    "         \n",
    "         \n",
    "         )\n",
    "    \n",
    "    if file.find(\"RED\") == 0:\n",
    "        total_red += 1\n",
    "        if \"{:.2f}\".format(prediction[0][0]*100) > (\"{:.2f}\".format(prediction[0][1]*100)):\n",
    "            total_red_correct_predict += 1\n",
    "\n",
    "    if file.find(\"GREEN\") == 0:\n",
    "        total_green += 1\n",
    "        if \"{:.2f}\".format(prediction[0][1]*100) > (\"{:.2f}\".format(prediction[0][0]*100)):\n",
    "            total_green_correct_predict += 1\n",
    "        \n",
    "    #if file.find(\"yellow\"):\n",
    "        #total_yellow += 1\n",
    "        #if \"{:.2f}\".format(prediction[0][2]*100) > (\"{:.2f}\".format(prediction[0][1]*100) + \"{:.2f}\".format(prediction[0][1]*100)):\n",
    "            #total_yellow_correct_predict += 1\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Red Accuracy:\", (total_red_correct_predict/total_red)*100,\"%\")\n",
    "print(\"Green Accuracy:\", (total_green_correct_predict/total_green)*100,\"%\")\n",
    "#print(\"Yellow Accuracy:\" , (total_yellow_correct_predict/total_yellow)*100)\n",
    "\n",
    "accuracy_total['Red'] += (total_red_correct_predict/total_red)*100\n",
    "accuracy_total['Green'] += (total_green_correct_predict/total_green)*100\n",
    "#accuracy_total['Yellow'] += (total_yellow_correct_predict/total_yellow)*100\n",
    "\n",
    "\n",
    "#Let's see it in a graph\n",
    "plt.bar(range(len(accuracy_total)), list(accuracy_total.values()), align='center')\n",
    "plt.xticks(range(len(accuracy_total)), list(accuracy_total.keys()))\n",
    "\n",
    "print(\"\\nAccuracy By Light Color:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring it all together... This takes about 40 minutes, sooo...dont run it unless you have to. \n",
    "# or at least remove some of the frames folders\n",
    "# This one is a bit different as it will only allow a max of 14681 red images and 14681 green images\n",
    "\n",
    "frame_path_arr = []\n",
    "annotations = []\n",
    "images = []\n",
    "labels = []\n",
    "color_space = [(0,255,0),(255,0,0),(255,0,0)]\n",
    "\n",
    "def process_images():\n",
    "    CUR_GR_IMG = 0\n",
    "    CUR_RE_IMG = 0\n",
    "    CUR_YE_IMG = 0\n",
    "    IMG_MAX = 14681\n",
    "\n",
    "    for folder in os.listdir(\"./data/dayTrain/dayTrain/\"):\n",
    "        subfolder = os.listdir(\"./data/dayTrain/dayTrain/\"+folder+\"/\"+\"/frames\")\n",
    "        \n",
    "        for file in subfolder:\n",
    "            frame_path_arr.append(\"./data/dayTrain/dayTrain/\"+folder+\"/\"+\"/frames/\"+file)\n",
    "\n",
    "    for img_path in frame_path_arr:\n",
    "        frame_name = img_path[::-1][0:img_path[::-1].index(\"/\")][::-1]\n",
    "        #print(frame_name)\n",
    "        frame_anno = \"./data/Annotations/Annotations/dayTrain\"+\"/\"+img_path[25:34].replace(\"/\",\"\")+\"/frameAnnotationsBOX.csv\"\n",
    "        #print(frame_anno)\n",
    "        \n",
    "        with open(frame_anno) as fa:  \n",
    "            line = fa.readline()\n",
    "            line = fa.readline()\n",
    "            while line:\n",
    "                anno_file_path = (line.strip()).split(\";\")\n",
    "                anno_file_id = anno_file_path[0].split(\"/\")[1]\n",
    "                #print(anno_file_path, anno_file_id)\n",
    "                #print(anno_file_id, frame_name)\n",
    "                if anno_file_id == frame_name:\n",
    "                    annotations.append(anno_file_path)\n",
    "                line = fa.readline()\n",
    "\n",
    "    for img_path in frame_path_arr:\n",
    "        for anno in annotations:\n",
    "            \n",
    "            if CUR_RE_IMG >= IMG_MAX  and CUR_GR_IMG >= IMG_MAX:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                if category == \"warningLeft\":\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if category == \"warning\":\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            #print(img_path[::-1][0:img_path[::-1].index(\"/\")][::-1])\n",
    "            #print(str(anno).split(\",\")[0].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"dayTraining/\", \"\"))\n",
    "            if img_path[::-1][0:img_path[::-1].index(\"/\")][::-1] == str(anno).split(\",\")[0].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"dayTraining/\", \"\"):\n",
    "                category = anno[1]\n",
    "                left = int(anno[2])\n",
    "                top = int(anno[3])\n",
    "                right = int(anno[4])\n",
    "                bottom = int(anno[5])\n",
    "                #print(\"file:\", img_path, \"\\tCategory: '{}' at [{},{},{},{}]\".format(category, left, top, right, bottom))\n",
    "                img = cv2.imread(img_path)\n",
    "                #img = cv2.resize(img, (25, 25))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                #plt.rcParams['figure.figsize'] = [32, 16]\n",
    "                cropped_image = img[top:bottom, left:right]\n",
    "                cropped_image = cv2.resize(cropped_image, (25, 25))\n",
    "                \n",
    "                try:\n",
    "                    if category == \"stop\" and CUR_RE_IMG < IMG_MAX:\n",
    "                        images.append(cropped_image)\n",
    "                        labels.append(0)\n",
    "                        CUR_RE_IMG += 1\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"go\" and CUR_GR_IMG < IMG_MAX:\n",
    "                        images.append(cropped_image)\n",
    "                        labels.append(1)\n",
    "                        CUR_GR_IMG += 1\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"stopLeft\" and CUR_RE_IMG < IMG_MAX:\n",
    "                        images.append(cropped_image)\n",
    "                        labels.append(0)\n",
    "                        CUR_RE_IMG += 1\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if category == \"goLeft\" and CUR_GR_IMG < IMG_MAX:\n",
    "                        images.append(cropped_image)\n",
    "                        labels.append(1)\n",
    "                        CUR_GR_IMG += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                print(\"Working on: \", img_path[::-1][0:img_path[::-1].index(\"/\")][::-1])\n",
    "                # --- uncomment to show images ---\n",
    "                #cv2.imshow(\"cropped\", cropped_image)\n",
    "                #plt.imshow(cropped_image)\n",
    "                #plt.show()   \n",
    "                \n",
    "\n",
    "process_images()\n",
    "\n",
    "data = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
